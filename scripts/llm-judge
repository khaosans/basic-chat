#!/bin/bash

# Quick LLM Judge - One-liner for local development
# Usage: ./scripts/llm-judge [quick|full] [auto|ollama|openai]

set -e

MODE=${1:-"quick"}
BACKEND=${2:-"auto"}

echo "ü§ñ LLM Judge - $MODE mode with $BACKEND backend"
echo "================================================"

# Check if we're in the right directory
if [ ! -f "pyproject.toml" ]; then
    echo "‚ùå Please run from the BasicChat root directory"
    exit 1
fi

# Force backend if specified (not auto)
if [ "$BACKEND" != "auto" ]; then
    export LLM_JUDGE_FORCE_BACKEND=$(echo $BACKEND | tr '[:lower:]' '[:upper:]')
fi

# Run the evaluation with smart backend selection
poetry run python basicchat/evaluation/evaluators/check_llm_judge_smart.py $([ "$MODE" = "quick" ] && echo "--quick")

# Generate action items if evaluation succeeded
if [ $? -eq 0 ] && [ -f "llm_judge_results.json" ]; then
    echo ""
    echo "üìã Generating action items..."
    poetry run python scripts/generate_llm_judge_report.py
    echo ""
    echo "‚úÖ Check llm_judge_action_items.md for improvements"
fi
