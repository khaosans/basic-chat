name: verifyExpected

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11]
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio
      - name: Run tests
        run: |
          pytest tests/ -v --tb=short
      - name: Run basic functionality test
        run: |
          python -c "
          try:
              from app import OllamaChat
              print('‚úÖ OllamaChat imported successfully')
              
              from reasoning_engine import ReasoningEngine
              print('‚úÖ ReasoningEngine imported successfully')
              
              from document_processor import DocumentProcessor
              print('‚úÖ DocumentProcessor imported successfully')
              
              from utils.enhanced_tools import EnhancedCalculator
              print('‚úÖ EnhancedCalculator imported successfully')
              
              print('‚úÖ All core modules imported successfully')
          except ImportError as e:
              print(f'‚ùå Import failed: {str(e)}')
              raise
          "
      - name: Test configuration
        run: |
          python -c "
          try:
              from config import config
              print(f'‚úÖ Configuration loaded: {config.ollama_model}')
              print(f'‚úÖ Ollama URL: {config.ollama_url}')
              print(f'‚úÖ Caching enabled: {config.enable_caching}')
          except Exception as e:
              print(f'‚ùå Configuration test failed: {str(e)}')
              raise
          "

  llm-judge:
    runs-on: ubuntu-latest
    # Run in parallel, don't wait for tests
    # Only run on pushes to main or trusted PRs (same repository)
    if: |
      (github.event_name == 'push' && github.ref == 'refs/heads/main') ||
      (github.event_name == 'pull_request' && github.event.pull_request.head.repo.full_name == github.repository)
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio requests azure-ai-inference
      - name: Run GitHub Models LLM Judge Evaluator
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_MODEL: ${{ vars.GITHUB_MODEL || 'microsoft/phi-3.5-mini' }}
          LLM_JUDGE_THRESHOLD: ${{ vars.LLM_JUDGE_THRESHOLD || '7.0' }}
        run: |
          set +e
          echo "üîÑ Attempting GitHub Models evaluation..."
          python evaluators/check_llm_judge_github.py --quick
          github_status=$?
          set -e
          
          if [ $github_status -ne 0 ]; then
            echo "‚ùå GitHub Models evaluation failed, trying OpenAI fallback..."
            
            # Check if OpenAI API key is available
            if [ -n "${{ secrets.OPENAI_API_KEY }}" ]; then
              echo "üîë OpenAI API key found, running OpenAI evaluator..."
              python evaluators/check_llm_judge_openai.py --quick
              openai_status=$?
              
              if [ $openai_status -ne 0 ]; then
                echo "‚ùå OpenAI evaluation also failed, using mock evaluation..."
                python evaluators/check_llm_judge.py --mock
              fi
            else
              echo "‚ö†Ô∏è  No OpenAI API key found, using mock evaluation..."
              python evaluators/check_llm_judge.py --mock
            fi
          fi
      - name: Upload LLM Judge Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: llm-judge-results
          path: llm_judge_results.json
          retention-days: 30
