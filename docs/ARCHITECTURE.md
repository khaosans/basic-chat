# System Architecture

This document provides a high-level overview of the technical architecture of BasicChat. It outlines the main components and the flow of data through the system.

[‚Üê Back to README](../README.md)

## üèõÔ∏è Core Architecture

The application is designed with a modular, layered architecture that separates the user interface, application logic, and AI processing.

```mermaid
graph TD
    subgraph User Interface
        A["Streamlit UI"]
    end

    subgraph Application Logic
        B["App Controller (app.py)"]
        C["Reasoning Engine"]
        D["Document Processor"]
        E["Configuration"]
    end

    subgraph Core Services
        F["Async Ollama Client"]
        G["ChromaDB Vector Store"]
        H["Tool Registry (Web, Calc)"]
        I["Caching (Redis/Memory)"]
    end
    
    subgraph External
        J["Ollama (Local LLMs)"]
    end

    A -- "User Input / File Upload" --> B
    B -- "Process Query" --> C
    B -- "Process File" --> D
    
    C -- "Uses Tools" --> H
    C -- "Sends LLM Request" --> F
    D -- "Manages Embeddings" --> G
    
    F -- "Talks to" --> J
    F -- "Checks/Stores Cache" --> I
    
    J -- "LLM Response" --> F
    F -- "Returns Response" --> C
    C -- "Final Answer" --> B
    B -- "Displays to" --> A
    
    E -- "Provides Settings" --> B
    E -- "Provides Settings" --> C
    E -- "Provides Settings" --> D
```

## üß© Key Components

-   **Streamlit UI (`app.py`)**: The main web interface that captures user input, handles file uploads, and displays the AI's responses. It acts as the primary controller for the application.

-   **Reasoning Engine (`reasoning_engine.py`)**: This is the brain of the application. It processes user queries, determines which reasoning mode to use (e.g., Chain-of-Thought), and interacts with tools (like the calculator or web search) and the LLM to formulate an answer.

-   **Document Processor (`document_processor.py`)**: Manages the lifecycle of uploaded documents. It handles:
    -   Extracting text from PDFs and images (OCR).
    -   Splitting documents into manageable chunks.
    -   Creating vector embeddings using a local model.
    -   Storing and retrieving embeddings from the ChromaDB vector store for RAG.
    -   Managing the cleanup of database directories.

-   **Async Ollama Client (`utils/async_ollama.py`)**: A high-performance, asynchronous client for communicating with the local Ollama LLM server. It includes connection pooling and caching to ensure fast and reliable responses.

-   **ChromaDB Vector Store**: A local vector database used to store document embeddings. The Document Processor uses it to perform semantic searches and retrieve relevant context for Retrieval-Augmented Generation (RAG).

-   **Configuration (`config.py`)**: A centralized module that manages all application settings, such as which AI models to use and caching configurations.

## üîÑ Data Flow for Document Analysis (RAG)

1.  A user uploads a document (e.g., a PDF) via the Streamlit UI.
2.  The `Document Processor` takes the file, extracts its text, and splits it into smaller chunks.
3.  Each chunk is converted into a numerical representation (an embedding) by the `nomic-embed-text` model.
4.  These embeddings are stored in a dedicated collection within the `ChromaDB` vector store.
5.  When the user asks a question about the document, the `Reasoning Engine` converts the question into an embedding.
6.  It then queries `ChromaDB` to find the most relevant text chunks from the document.
7.  This retrieved context is combined with the user's question and sent to the `mistral` LLM to generate a final, context-aware answer.

## Performance Architecture

### Async Processing
- **Connection Pooling**: 100 total connections, 30 per host for optimal resource utilization
- **Rate Limiting**: Configurable (default: 10 req/sec) with token bucket algorithm
- **Retry Logic**: Exponential backoff with 3 attempts for fault tolerance
- **Health Monitoring**: Real-time service availability checks with automatic failover

### Caching Strategy
- **Multi-layer**: Redis primary + Memory fallback for distributed and local caching
- **Smart Keys**: MD5 hash with parameter inclusion for collision resistance
- **Performance**: 50-80% faster response times with intelligent cache management
- **Hit Rate**: 70-85% for repeated queries with optimal key design

### Memory Management
- **Session State**: Streamlit session management with automatic cleanup
- **Vector Store**: ChromaDB with configurable persistence and memory limits
- **Cache Limits**: Configurable TTL and size limits with automatic eviction
- **Resource Cleanup**: Automatic cleanup and garbage collection for optimal performance

## Security Architecture

### Input Validation
- **Expression Sanitization**: Safe mathematical operations with dangerous operation detection
- **File Upload Security**: Type validation and size limits with malicious file detection
- **Rate Limiting**: Per-user/IP request throttling to prevent abuse and DDoS attacks
- **Error Handling**: Graceful degradation and fallbacks with actionable error messages

### Data Privacy
- **Local Processing**: All data processed locally via Ollama with no external API calls
- **No External Storage**: No data sent to external services except for web search queries
- **Configurable Logging**: Optional structured logging with privacy-preserving defaults
- **Session Isolation**: User session separation with no cross-user data access

## Scalability Considerations

### Horizontal Scaling
- **Stateless Design**: Session state in Streamlit with no server-side state dependencies
- **Redis Integration**: Distributed caching support for multi-instance deployments
- **Load Balancing**: Ready for multiple instances with health check integration
- **Health Checks**: Service availability monitoring with automatic failover

### Performance Optimization
- **Async Operations**: Non-blocking request handling with efficient resource utilization
- **Connection Reuse**: HTTP connection pooling for reduced latency and overhead
- **Batch Processing**: Efficient document chunking and embedding generation
- **Memory Management**: Configurable cache sizes with optimal eviction policies

## Technology Stack

### Core Technologies
- **Python 3.11+**: Main programming language with modern async/await support
- **Streamlit**: Web application framework for rapid UI development
- **Ollama**: Local LLM server for privacy-preserving AI inference
- **ChromaDB**: Vector database for semantic search and similarity matching

### Key Libraries
- **aiohttp**: Async HTTP client/server for high-performance networking
- **LangChain**: LLM application framework for AI system development
- **Pydantic**: Data validation and settings management with type safety
- **pytest**: Testing framework with comprehensive test coverage

### External Services
- **DuckDuckGo**: Web search (no API key required) for real-time information
- **Redis**: Optional distributed caching for multi-instance deployments
- **Tesseract**: OCR for image processing and text extraction

## üîó Related Documentation

- **[Installation Guide](INSTALLATION.md)** - Setup and configuration
- **[Features Overview](FEATURES.md)** - Detailed feature documentation
- **[Development Guide](DEVELOPMENT.md)** - Contributing and development
- **[Production Roadmap](ROADMAP.md)** - Future development plans
- **[Reasoning Features](../REASONING_FEATURES.md)** - Advanced reasoning engine details

## üìö References

### Research Papers
- **Chain-of-Thought Reasoning**: Wei et al. demonstrate that step-by-step reasoning significantly improves AI performance on complex tasks (Wei et al. 2201.11903).
- **Retrieval-Augmented Generation**: Lewis et al. introduce RAG as a method to enhance language models with external knowledge (Lewis et al. 2005.11401).
- **Vector Similarity Search**: Johnson et al. provide comprehensive analysis of approximate nearest neighbor search methods (Johnson et al. 1908.10396).

### Academic References
- **Distributed Systems**: Tanenbaum and van Steen provide comprehensive coverage of distributed system principles and practices (Tanenbaum and van Steen 2007).
- **Web Application Architecture**: Fielding and Reschke document HTTP protocol and web application design principles (Fielding and Reschke 2014).
- **Software Testing**: Myers et al. present comprehensive software testing methodologies and best practices (Myers et al. 2011).

### Core Technologies
- **Ollama**: [https://ollama.ai](https://ollama.ai) - Local large language model server
- **Streamlit**: [https://streamlit.io](https://streamlit.io) - Web application framework
- **LangChain**: [https://langchain.com](https://langchain.com) - LLM application framework
- **ChromaDB**: [https://chromadb.ai](https://chromadb.ai) - Vector database

### Works Cited
Wei, Jason, et al. "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models." *arXiv preprint arXiv:2201.11903*, 2022.

Lewis, Mike, et al. "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." *Advances in Neural Information Processing Systems*, vol. 33, 2020, pp. 9459-9474.

Johnson, Jeff, et al. "Billion-Scale Similarity Search with GPUs." *arXiv preprint arXiv:1908.10396*, 2019.

Fowler, Martin. *Microservices: A Definition of This New Architectural Term*. Martin Fowler, 2014, martinfowler.com/articles/microservices.html.

Tanenbaum, Andrew S., and Maarten van Steen. *Distributed Systems: Principles and Paradigms*. 2nd ed., Prentice Hall, 2007.

Fielding, Roy T., and Julian F. Reschke. "Hypertext Transfer Protocol (HTTP/1.1): Authentication." *Internet Engineering Task Force*, RFC 7235, 2014.

Myers, Glenford J., et al. *The Art of Software Testing*. 3rd ed., John Wiley & Sons, 2011.

---

[‚Üê Back to README](../README.md) | [Installation ‚Üê](INSTALLATION.md) | [Features ‚Üê](FEATURES.md) | [Development ‚Üí](DEVELOPMENT.md) | [Roadmap ‚Üí](ROADMAP.md) 