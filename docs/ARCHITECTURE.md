# System Architecture

This document provides a high-level overview of the technical architecture of BasicChat. It outlines the main components and the flow of data through the system.

[‚Üê Back to README](../README.md)

---

## üèõÔ∏è Core Architecture

The application is designed with a modular, layered architecture that separates the user interface, application logic, and core services. This separation makes the system easier to develop, test, and maintain.

```mermaid
graph TD
    classDef ui fill:#87CEEB,stroke:#4682B4,color:#000
    classDef logic fill:#98FB98,stroke:#3CB371,color:#000
    classDef service fill:#F0E68C,stroke:#BDB76B,color:#000
    classDef external fill:#D3D3D3,stroke:#A9A9A9,color:#000

    subgraph User Facing
        A["<br/><b>Streamlit UI</b><br/>User interaction and file uploads<br/>"]:::ui
    end

    subgraph Application Logic
        B["<b>App Controller</b><br/>(app.py)<br/>Routes user input"]:::logic
        C["<b>Reasoning Engine</b><br/>Determines logic and uses tools"]:::logic
        D["<b>Document Processor</b><br/>Manages document lifecycle and RAG"]:::logic
    end

    subgraph Core Services
        F["<b>Async Ollama Client</b><br/>Communicates with LLMs"]:::service
        G["<b>ChromaDB Vector Store</b><br/>Stores and retrieves document embeddings"]:::service
        H["<b>Tool Registry</b><br/>Provides Calculator, Web Search, etc."_]:::service
        I["<b>Caching Service</b><br/>In-memory & Redis for performance"]:::service
    end
    
    subgraph External Systems
        J["<b>Ollama Service</b><br/>Hosts local LLMs"]:::external
    end

    A -- "User Query / File" --> B
    
    B -- "Processes Query" --> C
    B -- "Processes File" --> D
    
    C -- "Uses" --> H
    C -- "Sends Request" --> F
    
    D -- "Manages" --> G
    
    F -- "Queries" --> J
    F -- "Caches" --> I
    
    J -- "LLM Response" --> F
    F -- "Returns Response" --> C
    C -- "Final Answer" --> B
    B -- "Displays Result" --> A
```

## üß© Key Components

-   **Streamlit UI (`app.py`)**: The main web interface that captures user input, handles file uploads, and displays the AI's responses.
-   **Reasoning Engine (`reasoning_engine.py`)**: The core logic unit. It interprets user queries, decides which reasoning mode to use, and coordinates with tools and the LLM to generate an answer.
-   **Document Processor (`document_processor.py`)**: Manages the entire lifecycle of documents, from text extraction and embedding to storage and retrieval for RAG.
-   **Core Services**: A set of internal services that provide essential functionalities:
    -   `Async Ollama Client`: A high-performance client for non-blocking communication with the Ollama server.
    -   `ChromaDB Vector Store`: A local database for efficient semantic search over documents.
    -   `Tool Registry`: Provides access to built-in capabilities like the calculator and web search.
    -   `Caching`: A multi-layer cache to accelerate responses.

---

## üîÑ Data Flows

The following diagrams illustrate how data moves through the system for different scenarios.

### Standard Query with Tool Use

This flow shows how the system answers a question like *"What is the square root of 144?"* using the **Agent-Based** reasoning mode.

```mermaid
sequenceDiagram
    participant User
    participant UI as Streamlit UI
    participant Engine as Reasoning Engine
    participant Tools as Tool Registry
    participant LLM as Ollama LLM
    
    User->>+UI: Asks "sqrt(144)?"
    UI->>+Engine: Process query
    Engine->>+LLM: "Should I use a tool for 'sqrt(144)'?"
    LLM-->>-Engine: "Yes, use calculator"
    Engine->>+Tools: Execute calculator with "sqrt(144)"
    Tools-->>-Engine: Return "12"
    Engine->>+LLM: "The tool returned 12. Formulate the final answer."
    LLM-->>-Engine: "The square root of 144 is 12."
    Engine-->>-UI: Send final answer
    UI-->>-User: Display "The square root of 144 is 12."
```

### Document Analysis (RAG)

This flow shows how the system answers a question based on an uploaded document.

```mermaid
sequenceDiagram
    participant User
    participant UI as Streamlit UI
    participant DocProc as Document Processor
    participant DB as ChromaDB
    participant Engine as Reasoning Engine
    participant LLM as Ollama LLM

    User->>+UI: Uploads "research_paper.pdf"
    UI->>+DocProc: Process file
    DocProc->>+DB: Extract text, create embeddings,<br/>and store in database
    DB-->>-DocProc: Confirm storage
    DocProc-->>-UI: Notify success

    User->>+UI: Asks "What was the main finding?"
    UI->>+Engine: Process query with document context
    Engine->>+DB: Search for relevant chunks in "research_paper.pdf"
    DB-->>-Engine: Return top 3 relevant text chunks
    Engine->>+LLM: "Context: [text chunks]<br/>Question: What was the main finding?"
    LLM-->>-Engine: Generate answer based on context
    Engine-->>-UI: Send final answer
    UI-->>-User: Display answer
```

## Performance Architecture

### Async Processing
- **Connection Pooling**: 100 total connections, 30 per host for optimal resource utilization
- **Rate Limiting**: Configurable (default: 10 req/sec) with token bucket algorithm
- **Retry Logic**: Exponential backoff with 3 attempts for fault tolerance
- **Health Monitoring**: Real-time service availability checks with automatic failover

### Caching Strategy
- **Multi-layer**: Redis primary + Memory fallback for distributed and local caching
- **Smart Keys**: MD5 hash with parameter inclusion for collision resistance
- **Performance**: 50-80% faster response times with intelligent cache management
- **Hit Rate**: 70-85% for repeated queries with optimal key design

### Memory Management
- **Session State**: Streamlit session management with automatic cleanup
- **Vector Store**: ChromaDB with configurable persistence and memory limits
- **Cache Limits**: Configurable TTL and size limits with automatic eviction
- **Resource Cleanup**: Automatic cleanup and garbage collection for optimal performance

## Security Architecture

### Input Validation
- **Expression Sanitization**: Safe mathematical operations with dangerous operation detection
- **File Upload Security**: Type validation and size limits with malicious file detection
- **Rate Limiting**: Per-user/IP request throttling to prevent abuse and DDoS attacks
- **Error Handling**: Graceful degradation and fallbacks with actionable error messages

### Data Privacy
- **Local Processing**: All data processed locally via Ollama with no external API calls
- **No External Storage**: No data sent to external services except for web search queries
- **Configurable Logging**: Optional structured logging with privacy-preserving defaults
- **Session Isolation**: User session separation with no cross-user data access

## Scalability Considerations

### Horizontal Scaling
- **Stateless Design**: Session state in Streamlit with no server-side state dependencies
- **Redis Integration**: Distributed caching support for multi-instance deployments
- **Load Balancing**: Ready for multiple instances with health check integration
- **Health Checks**: Service availability monitoring with automatic failover

### Performance Optimization
- **Async Operations**: Non-blocking request handling with efficient resource utilization
- **Connection Reuse**: HTTP connection pooling for reduced latency and overhead
- **Batch Processing**: Efficient document chunking and embedding generation
- **Memory Management**: Configurable cache sizes with optimal eviction policies

## Technology Stack

### Core Technologies
- **Python 3.11+**: Main programming language with modern async/await support
- **Streamlit**: Web application framework for rapid UI development
- **Ollama**: Local LLM server for privacy-preserving AI inference
- **ChromaDB**: Vector database for semantic search and similarity matching

### Key Libraries
- **aiohttp**: Async HTTP client/server for high-performance networking
- **LangChain**: LLM application framework for AI system development
- **Pydantic**: Data validation and settings management with type safety
- **pytest**: Testing framework with comprehensive test coverage

### External Services
- **DuckDuckGo**: Web search (no API key required) for real-time information
- **Redis**: Optional distributed caching for multi-instance deployments
- **Tesseract**: OCR for image processing and text extraction

## üîó Related Documentation

- **[Installation Guide](INSTALLATION.md)** - Setup and configuration
- **[Features Overview](FEATURES.md)** - Detailed feature documentation
- **[Development Guide](DEVELOPMENT.md)** - Contributing and development
- **[Production Roadmap](ROADMAP.md)** - Future development plans
- **[Reasoning Features](../REASONING_FEATURES.md)** - Advanced reasoning engine details

## üìö References

### Research Papers
- **Chain-of-Thought Reasoning**: Wei et al. demonstrate that step-by-step reasoning significantly improves AI performance on complex tasks (Wei et al. 2201.11903).
- **Retrieval-Augmented Generation**: Lewis et al. introduce RAG as a method to enhance language models with external knowledge (Lewis et al. 2005.11401).
- **Vector Similarity Search**: Johnson et al. provide comprehensive analysis of approximate nearest neighbor search methods (Johnson et al. 1908.10396).

### Academic References
- **Distributed Systems**: Tanenbaum and van Steen provide comprehensive coverage of distributed system principles and practices (Tanenbaum and van Steen 2007).
- **Web Application Architecture**: Fielding and Reschke document HTTP protocol and web application design principles (Fielding and Reschke 2014).
- **Software Testing**: Myers et al. present comprehensive software testing methodologies and best practices (Myers et al. 2011).

### Core Technologies
- **Ollama**: [https://ollama.ai](https://ollama.ai) - Local large language model server
- **Streamlit**: [https://streamlit.io](https://streamlit.io) - Web application framework
- **LangChain**: [https://langchain.com](https://langchain.com) - LLM application framework
- **ChromaDB**: [https://chromadb.ai](https://chromadb.ai) - Vector database

### Works Cited
Wei, Jason, et al. "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models." *arXiv preprint arXiv:2201.11903*, 2022.

Lewis, Mike, et al. "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." *Advances in Neural Information Processing Systems*, vol. 33, 2020, pp. 9459-9474.

Johnson, Jeff, et al. "Billion-Scale Similarity Search with GPUs." *arXiv preprint arXiv:1908.10396*, 2019.

Fowler, Martin. *Microservices: A Definition of This New Architectural Term*. Martin Fowler, 2014, martinfowler.com/articles/microservices.html.

Tanenbaum, Andrew S., and Maarten van Steen. *Distributed Systems: Principles and Paradigms*. 2nd ed., Prentice Hall, 2007.

Fielding, Roy T., and Julian F. Reschke. "Hypertext Transfer Protocol (HTTP/1.1): Authentication." *Internet Engineering Task Force*, RFC 7235, 2014.

Myers, Glenford J., et al. *The Art of Software Testing*. 3rd ed., John Wiley & Sons, 2011.

---

[‚Üê Back to README](../README.md) | [Installation ‚Üê](INSTALLATION.md) | [Features ‚Üê](FEATURES.md) | [Development ‚Üí](DEVELOPMENT.md) | [Roadmap ‚Üí](ROADMAP.md) 